---
title: Lecture 05 - Conjugate Gradient Method (CGM)
subtitle: In this lecture, we introduce the conjugate gradient method (CGM) for solving the system of linear equations.
layout: default
date: 2025-10-12
keywords: CGM, linear system, numerical computation
published: true
---

In this lecture, we introduce the conjugate gradient method (CGM) for solving ${\mathbf A}{\mathbf x} = {\mathbf b}$ where $\mathbf A$ is a symmetric positive definite (s.p.d.) matrix. Our motivation starts from the Steepest Descent (SD) method introduced in our previous lecture, which admits ${\mathcal{O}\!\left(\kappa({\mathbf A}) \log \tfrac{1}{\epsilon}\!\right)}$ iteration complexity for achieving $||{\mathbf x}_t - {\mathbf x}^*|| \leq \epsilon$. We will see how the conjugation constructed from residuals helps to improve SD and leads to the CGM method. This method admits ${\mathcal{O}}\!\left(\sqrt{\kappa(\mathbf A)}\log \tfrac{1}{\epsilon}\!\right)$ iteration complexity. Much of this note is adopted mainly from the excellent tutorial {% cite shewchuk1994introduction %} and {% cite kelley1995iterative %}.


## 1. Preliminaries and Steepest Descent (SD)

First, we recall some notations and review the Steepest Descent (SD) method from our previous lecture. We then comment on why a better iterative method might be possible.

**Notations**. We denote a set of all s.p.d. real matrices as ${\mathbb S}_{++}^n$. Throught out of this lecture, ${\mathbf A} \in {\mathbb S}_{++}^n$. The eigenvalues of ${\mathbf A}$ is ordered as $0 < \lambda_1 \leq \lambda_2 \leq \cdots \leq \lambda_n$. We denote the spectrum of ${\mathbf A}$ as $\sigma({\mathbf A}) = \{\lambda_1,\ldots,\lambda_n\}$. The condition number of ${\mathbf A}$ is defined as $\kappa({\mathbf A}) = \|{\mathbf A}^{-1}\|_2\cdot \|{\mathbf A}\|_2 = \lambda_n/\lambda_1$. The unique solution of ${\mathbf A} {\mathbf x} = {\mathbf b}$ is denoted as ${\mathbf x}^* = {\mathbf A}^{-1}{\mathbf b}$. We consider an iterative procedure that generates $\{{{\mathbf x}}_{t}\} \rightarrow {\mathbf x}^*$ with an arbitrary initial guess ${\mathbf x}_0$. The estimation error is ${\mathbf e}_t = {\mathbf x}_t - {\mathbf x}^*$ to measure the progress. The residual is ${\mathbf r}_t = {\mathbf b} - {\mathbf A} {\mathbf x}_t$ and ${\mathbf r}_t = \v0$ if and only if ${\mathbf x}_t = {\mathbf x}^*$. Clearly, ${\mathbf r}_t = -{\mathbf A}{\mathbf e}_t$. Given a differentiable function $f: \R^n \rightarrow \R$, we denote its gradient at ${\mathbf x}$ as $\bm \nabla f({\mathbf x})$. The ${\mathbf A}$-induced norm is defined as $\|{\mathbf x}\|_{{\mathbf A}} = \sqrt{\langle {\mathbf x}, {\mathbf A} {\mathbf x} \rangle } $.