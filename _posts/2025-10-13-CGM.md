---
title: Lecture 05 - Conjugate Gradient Method (CGM)
subtitle: In this lecture, we introduce the conjugate gradient method (CGM) for solving the system of linear equations.
layout: default
date: 2025-10-12
keywords: CGM, linear system, numerical computation
published: true
---

In this lecture, we introduce the conjugate gradient method (CGM) for solving ${\mathbf A}{\mathbf x} = {\mathbf b}$ where $\mathbf A$ is a symmetric positive definite (s.p.d.) matrix. Our motivation starts from the Steepest Descent (SD) method introduced in our previous lecture, which admits ${\mathcal{O}\!\left(\kappa({\mathbf A}) \log \tfrac{1}{\epsilon}\!\right)}$ iteration complexity for achieving $||{\mathbf x}_t - {\mathbf x}^*|| \leq \epsilon$. We will see how the conjugation constructed from residuals helps to improve SD and leads to the CGM method. This method admits ${\mathcal{O}}\!\left(\sqrt{\kappa(\mathbf A)}\log \tfrac{1}{\epsilon}\!\right)$ iteration complexity. Much of this note is adopted mainly from the excellent tutorial {% cite shewchuk1994introduction %} and {% cite kelley1995iterative %}.


## 1. Preliminaries and Steepest Descent (SD)

First, we recall some notations and review the Steepest Descent (SD) method from our previous lecture. We then comment on why a better iterative method might be possible.

**Notations**. We denote a set of all s.p.d. real matrices as ${\mathbb S}_{++}^n$. Throught out of this lecture, ${\mathbf A} \in {\mathbb S}_{++}^n$. The eigenvalues of ${\mathbf A}$ is ordered as $0 < \lambda_1 \leq \lambda_2 \leq \cdots \leq \lambda_n$. We denote the spectrum of ${\mathbf A}$ as $\sigma({\mathbf A}) = \{\lambda_1,\ldots,\lambda_n\}$. The condition number of ${\mathbf A}$ is defined as $\kappa({\mathbf A}) = \|{\mathbf A}^{-1}\|_2\cdot \|{\mathbf A}\|_2 = \lambda_n/\lambda_1$. The unique solution of ${\mathbf A} {\mathbf x} = {\mathbf b}$ is denoted as ${\mathbf x}^* = {\mathbf A}^{-1}{\mathbf b}$. We consider an iterative procedure that generates $\{{{\mathbf x}}_{t}\} \rightarrow {\mathbf x}^*$ with an arbitrary initial guess ${\mathbf x}_0$. The estimation error is ${\mathbf e}_t = {\mathbf x}_t - {\mathbf x}^*$ to measure the progress. The residual is ${\mathbf r}_t = {\mathbf b} - {\mathbf A} {\mathbf x}_t$ and ${\mathbf r}_t = \v0$ if and only if ${\mathbf x}_t = {\mathbf x}^*$. Clearly, ${\mathbf r}_t = -{\mathbf A}{\mathbf e}_t$. Given a differentiable function $f: \R^n \rightarrow \R$, we denote its gradient at ${\mathbf x}$ as $\bm \nabla f({\mathbf x})$. The ${\mathbf A}$-induced norm is defined as $\|{\mathbf x}\|_{{\mathbf A}} = \sqrt{\langle {\mathbf x}, {\mathbf A} {\mathbf x} \rangle } $.

Let's recall our problem. Given a  s.p.d. matrix ${\mathbf A} \in {\mathbb S}_{++}^n$ and ${\mathbf b} \in \R^n$, we want to solve the  linear system
$$
{\mathbf A} {\mathbf x} = {\mathbf b}, \tag{1}
$$
where ${\mathbf A}$ usually is large-scale and sparse. To solve \tag{1}, it is equivalent to minimizing a quadratic function $f({\mathbf x})$ defined as the following
$$
{\mathbf x}^* = \argmin_{{{\mathbf x}} \in \R^n} \left\{ f({\mathbf x}) := \frac{1}{2} {\mathbf x}^\top {\mathbf A} {\mathbf x} - {\mathbf x}^\top {\mathbf b} \right\}, \tag{3}
$$
where $f({\mathbf x})$ is strongly convex, hence ${\mathbf x}^*$ is unique and its gradient at this optimal point satisfying $\bm\nabla f({\mathbf x}^*) = \v0$, which is exactly ${\mathbf A} {\mathbf x}^* = {\mathbf b}$. If this is unfamiliar, consider ${\mathbf A} = a > 0$ and ${\mathbf b} = b$ in a single dimension. A standard iterative algorithm for solving \eqref{equ:1.2} is gradient descent
$$
{\mathbf x}_{t+1} = {\mathbf x}_t - \alpha_t \bm \nabla f({\mathbf x}_t) = {\mathbf x}_t + \alpha_t {\mathbf r}_t, \qquad t =0,1,2,\ldots,
$$
where the last equality due to $\bm\nabla f({\mathbf x}_t) = {\mathbf A} {\mathbf x}_t - {\mathbf b} = - {\mathbf r}_t$. The idea of the steepest descent method is a kind of *greedy* method: at each iteration, it finds a *local* best step size $\alpha_t$ choosing ${\mathbf x}_{t+1}$ such that $f({\mathbf x}_{t+1})$ can be minimized. This leads to $\alpha_t = \langle {\mathbf r}_t, {\mathbf r}_t\rangle / \langle {\mathbf r}_t, {\mathbf A} {\mathbf r}_t\rangle$, that is

$$
\text{Steepest Descent(SD)}: \quad {\mathbf x}_{t+1} = {\mathbf x}_t + \frac{\langle {\mathbf r}_t, {\mathbf r}_t\rangle}{\langle {\mathbf r}_t, {\mathbf A} {\mathbf r}_t \rangle} {\mathbf r}_t. \tag{2}
$$

To measure the progress of SD, we use the ${\mathbf A}$-induced norm of ${\mathbf e}_t$, i.e., $f({\mathbf x}_t) - f({\mathbf x}^*) = \tfrac{1}{2} {\|{\mathbf e}_t\|}_{{\mathbf A}}^2$. 

To see this, note, for any ${\mathbf x}$ and based on \tag{1}, we have the following identity

$$
f({\mathbf x}) = f({\mathbf x}^*) + \frac{1}{2}({\mathbf x} - {\mathbf x}^*)^\top {\mathbf A}({\mathbf x} - {\mathbf x}^*).
$$

Note ${\mathbf A}$ is s.p.d. and then $f({\mathbf x}) \geq f({\mathbf x}^*)$ for any ${\mathbf x}$. It attains the minimal when ${\mathbf x} = {\mathbf x}^*$. It is clear that $f({\mathbf x}_t) - f({\mathbf x}^*) = \|{\mathbf e}_t\|_{{\mathbf A}}^2/2 $.  We have the following convergence theorem for the Steepest Descent method.

>**Theorem 1: Convergence of SD**:
>Given any ${\mathbf x}_0, {\mathbf A} \in {\mathbb S}_{++}^n$, and ${\mathbf b} \in \R^n$, the SD method gaven in \tag{1} converges to $\bm x^* = {\mathbf A}^{-1}{\mathbf b}$ where the per-iteration update of ${\mathbf x}_{t+1}$ is orthongal, i.e., $\langle{\mathbf r}_t,{\mathbf r}_{t+1}\rangle = 0$. At each iteration $t$, the following inequality holds
>$$\|\bm e_{t}\|_{\bm A} \leq \left(\frac{\kappa(\bm A) - 1}{\kappa(\bm A) +1} \right)^t \|\bm e_{0}\|_{\bm A}, \qquad f({\mathbf x}_t) - f({\mathbf x}^*) \leq \left(\frac{\kappa(\bm A) - 1}{\kappa(\bm A) +1} \right)^{2 t} \left( f({\mathbf x}_0) - f({\mathbf x}^*) \right)
>$$
>The SD method needs at most ${\mathcal O}(\kappa({\mathbf A}) \log1/\epsilon)$ iterations for achieving $\|{\mathbf e}_t\|_{\mathbf A} \leq \epsilon$.

>Remark: Essentially, SD chooses $\alpha_t$ such that the following search directions (i.e., the gradient) are orthogonal to the previous one $\langle {\mathbf r}_t, {\mathbf r}_{t+1} \rangle = 0$ and $f({\mathbf x}_{t+1})$ is minimized per-iteration. We do not know exactly how many steps it will use to find ${\mathbf x}^*$, especially when $\kappa({\mathbf A})$ is large. The natural question is whether there is a better way to choose $\alpha_t$ and searching directions (other than gradients) so that ${\|{\mathbf e}_t\|}_{{\mathbf A}}$ is minimized (not $f({\mathbf x}_{t+1})$ at per-iteration) and can finish in at most $n$ iterations. The conjugate direction method serves as a general approach, with the conjugate gradient method (CGM) being a more efficient and specialized variant of it.

## 2. Conjugate Direction method (CD)


From the above preliminaries, we know that ${\mathbf x}_{t+1} = {\mathbf x}_{t} + \alpha_t {\mathbf r}_t$ may not be the optimal way to update $\{{\mathbf x}_t\}$ as it often finds itself taking steps in the same direction as earlier steps. From {% cite shewchuk1994introduction  %}: 

>Wouldn't it be better if, every time we took a step, we got it right the first time? Here's an idea: pick a set of orthogonal search directions ${\mathbf d}_{0:n-1}$. In each search direction, we'll take exactly one step, and that step will be just the right length to line up evenly with ${\mathbf x}$. After at most $n$ steps, we'll be done.